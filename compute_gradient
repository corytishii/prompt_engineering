# Completed compute_gradient function
def compute_gradient(x, y, w, b):
    """
    Computes the gradient for linear regression for the entire dataset.

    Args:
        x (ndarray): Shape (m,) Input data (Population of cities)
        y (ndarray): Shape (m,) Target values (Profits)
        w (scalar): Parameter for the weight (slope)
        b (scalar): Parameter for the bias (intercept)

    Returns:
        dj_dw (float): The gradient of the cost with respect to w
        dj_db (float): The gradient of the cost with respect to b
    """
    # Number of training examples
    m = x.shape[0]
    
    # Initialize gradients
    dj_dw = 0
    dj_db = 0
    
    ### START CODE HERE ###
    # Loop over all examples
    for i in range(m):
        # Compute the model's prediction for the ith example
        f_wb = w * x[i] + b
        
        # Compute the error for the ith example
        error = f_wb - y[i]
        
        # Compute the gradient for w and b from the ith example
        dj_dw_i = error * x[i]  # Gradient for w from ith example
        dj_db_i = error          # Gradient for b from ith example
        
        # Sum the gradients across all examples
        dj_dw += dj_dw_i
        dj_db += dj_db_i

    # Average the gradients over all examples
    dj_dw /= m
    dj_db /= m
    ### END CODE HERE ###
    
    return dj_dw, dj_db

# Checked the implementation of the function with two different initializations of the parameters w,b.
# Compute and display gradient with w initialized to zeroes
initial_w = 0
initial_b = 0

tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, initial_w, initial_b)
print('Gradient at initial w, b (zeros):', tmp_dj_dw, tmp_dj_db)

compute_gradient_test(compute_gradient)

# Compute and display cost and gradient with non-zero w
test_w = 0.2
test_b = 0.2
tmp_dj_dw, tmp_dj_db = compute_gradient(x_train, y_train, test_w, test_b)

print('Gradient at test w, b:', tmp_dj_dw, tmp_dj_db)
# Gradient at test w, b: -47.41610118114435 -4.007175051546391

